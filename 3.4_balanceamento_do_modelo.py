import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

# ğŸ“‚ Carregar os dados fusionados
print("ğŸ“‚ Carregando `base_fusionada.csv`...")
df = pd.read_csv("base_fusionada.csv", delimiter=";", encoding="utf-8")
print(f"âœ… Base carregada com {df.shape[0]} registros e {df.shape[1]} colunas.")

# ğŸ“Š SeleÃ§Ã£o de Features (adicionando novas variÃ¡veis)
features = [
    "PRECIPITAÃ‡ÃƒO TOTAL, HORÃRIO (mm)", 
    "TEMPERATURA DO AR - BULBO SECO, HORARIA (Â°C)",
    "UMIDADE RELATIVA DO AR, HORARIA (%)",
    "VENTO, VELOCIDADE HORARIA (m/s)",
    "PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)",
    "RADIACAO GLOBAL (Kj/mÂ²)"
]
target = "qtd_atividade_bin"

# ğŸš¨ Verificar valores ausentes
df.dropna(subset=features + [target], inplace=True)
print(f"âœ… ApÃ³s remoÃ§Ã£o de valores ausentes, restam {df.shape[0]} registros.")

# ğŸ—ï¸ SeparaÃ§Ã£o Treino/Teste
X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, stratify=df[target], random_state=42)

# ğŸ”„ Balanceamento de Classes
print("ğŸ”„ Aplicando SMOTE e undersampling para balanceamento...")
smote = SMOTE(sampling_strategy=0.5, random_state=42)
under_sampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
X_train_res, y_train_res = under_sampler.fit_resample(X_train_res, y_train_res)
print(f"âœ… Base balanceada: {X_train_res.shape[0]} registros apÃ³s SMOTE e undersampling.")

# ğŸ”„ NormalizaÃ§Ã£o\scaler = StandardScaler()
scaler = StandardScaler()  # ğŸ”¹ Definindo o scaler antes de usÃ¡-lo
X_train_res = scaler.fit_transform(X_train_res)
X_test = scaler.transform(X_test)

# ğŸ“Œ Modelos a testar
modelos = {
    "Random Forest": RandomForestClassifier(class_weight="balanced", random_state=42),
    "XGBoost": XGBClassifier(scale_pos_weight=len(y_train_res) / sum(y_train_res == 1), random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)
}

# ğŸ¯ Treinamento e AvaliaÃ§Ã£o
df_resultados = {}
print("\nğŸ”¬ Testando Modelos de Machine Learning...\n")
for nome, modelo in modelos.items():
    print(f"ğŸš€ Treinando Modelo: {nome}")
    modelo.fit(X_train_res, y_train_res)
    y_pred = modelo.predict(X_test)
    
    acuracia = np.mean(y_pred == y_test)
    print(f"ğŸ“Š AcurÃ¡cia: {acuracia:.4f}")
    print(f"\nğŸ“Š Matriz de ConfusÃ£o:")
    print(confusion_matrix(y_test, y_pred))
    print(f"\nğŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o:")
    print(classification_report(y_test, y_pred))
    
    df_resultados[nome] = {
        "AcurÃ¡cia": acuracia
    }

# ğŸ“Š ComparaÃ§Ã£o Final
resultados_df = pd.DataFrame(df_resultados).T
print("\nğŸ“Š ComparaÃ§Ã£o Final entre Modelos:")
print(resultados_df)

# ğŸ“ˆ GrÃ¡fico de ComparaÃ§Ã£o
plt.figure(figsize=(10, 5))
sns.barplot(x=resultados_df.index, y=resultados_df["AcurÃ¡cia"], palette="viridis")
plt.title("ComparaÃ§Ã£o de Modelos - AcurÃ¡cia")
plt.ylabel("AcurÃ¡cia")
plt.xticks(rotation=45)
plt.show()

print("âœ… Ajustes concluÃ­dos! O modelo estÃ¡ pronto para anÃ¡lise final.")
